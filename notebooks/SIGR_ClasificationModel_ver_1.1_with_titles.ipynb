{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aa52061",
   "metadata": {},
   "source": [
    "## CONVOLUTIONAL NEURAL NETWORK MODEL FOR WASTE CASSIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f516b223",
   "metadata": {},
   "source": [
    "## 1. Library Imports and Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4738a1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Image classification optimized to use more RAM\"\"\"\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#  Enable Mixed Precision to reduce VRAM consumption\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "#  Configure GPU to allow dynamic memory growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\" GPU configurada con crecimiento de memoria din√°mico\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3055b937",
   "metadata": {},
   "source": [
    "## 2. Dataset Loading and Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe36037f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Dataset configuration\n",
    "data_dir = pathlib.Path(\"/home/daniel/Documents/SIGR/SIRG DATASET/train\")\n",
    "if not data_dir.exists():\n",
    "    raise FileNotFoundError(f\"ERROR: no such directory: {data_dir}\")\n",
    "\n",
    "#  Image parameters\n",
    "batch_size = 20  #  increase batch size to use more RAM\n",
    "img_height = 400\n",
    "img_width = 400\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "#  Load datasets with higher RAM usage\n",
    "print(\" loading TRAINING DATASET...\")\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.1,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "print(\"\\n loading TESTING DATASET...\")\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.1,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "#  Obtener clases\n",
    "class_names = train_ds.class_names\n",
    "print(\"\\n CLASSES:\", class_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad48bd4",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da580263",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#  Configure dataset for performance **(more ram usage)**\n",
    "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y), num_parallel_calls=AUTOTUNE)\n",
    "val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y), num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(2000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1376dd",
   "metadata": {},
   "source": [
    "## 4. Data Augmentation Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f59255",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Data augmentation\n",
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\", input_shape=(img_height, img_width, 3)),\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomZoom(0.1),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b851cf",
   "metadata": {},
   "source": [
    "## 5. Optimized CNN Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3539f636",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Optimized model\n",
    "num_classes = len(class_names)\n",
    "model = Sequential([\n",
    "    data_augmentation,\n",
    "    layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(256, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    layers.Dropout(0.3),  # Deletes 30% of the neurons to prevent overfitting\n",
    "    layers.Dense(num_classes),\n",
    "\n",
    "])\n",
    "\n",
    "#  Compile the model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.build(input_shape=(None, img_height, img_width, 3))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd7a346",
   "metadata": {},
   "source": [
    "## 6. Model Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac83def",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "num_batches = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "print(f\" Total number of batches in train_ds: {num_batches}\")\n",
    "\n",
    "for image_batch, label_batch in train_ds.take(1):  # Take one batch for demonstration\n",
    "    print(f\" Batch size: {image_batch.shape}\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e625e96e",
   "metadata": {},
   "source": [
    "## 7. Model Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce5ab9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Model training\n",
    "epochs = 40\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d83f638",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation and Metric Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8266a625",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Save the model\n",
    "model.save('/home/daniel/Documents/SIGR/model_ver1.1.keras')\n",
    "\n",
    "#  Convert model to TensorFlow Lite (optimizado para float16)\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.allow_custom_ops = True\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.SELECT_TF_OPS]  #  Habilita TF Select\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "#  Save model in .tflite format\n",
    "tflite_path = \"/home/daniel/Documents/SIGR/model_ver1.1.tflite\"\n",
    "with open(tflite_path, \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(f\" Modelo guardado en: {tflite_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a209dad3",
   "metadata": {},
   "source": [
    "## 9. Saving the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c671b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Function for image prediction\n",
    "def predict_image(image_path):\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"X Image not found: {image_path}\")\n",
    "        return\n",
    "\n",
    "    img = keras.preprocessing.image.load_img(image_path, target_size=(img_height, img_width))\n",
    "    img_array = keras.preprocessing.image.img_to_array(img)\n",
    "\n",
    "    # Normalize image (as in training)\n",
    "    img_array = img_array / 255.0  # Normalizar a rango [0,1]\n",
    "\n",
    "    img_array = tf.expand_dims(img_array, 0)  # Create batch\n",
    "\n",
    "    predictions = model.predict(img_array)\n",
    "    score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "    print(f\"üîç The image is probably {class_names[np.argmax(score)]} with a confidence of {100 * np.max(score):.2f}%.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f45030b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Prediction test with an image\n",
    "test_image_path = \"/home/daniel/Documents/SIGR/Images/test/camisa.jfif\"\n",
    "predict_image(test_image_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faf8bb7",
   "metadata": {},
   "source": [
    "## 10. Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a29861f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Visualize training results\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
